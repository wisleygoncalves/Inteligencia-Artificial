# ğŸ¤– Curso de Machine Learning - MÃ³dulo de Algoritmos Supervisionados e NÃ£o Supervisionados

Este repositÃ³rio contÃ©m anotaÃ§Ãµes, cÃ³digos e explicaÃ§Ãµes dos principais algoritmos de Machine Learning abordados durante o curso. Abaixo vocÃª encontra a descriÃ§Ã£o teÃ³rica e prÃ¡tica de cada tema estudado.

---

## ğŸ“˜ Ãndice

1. [ğŸ“ˆ RegressÃ£o Linear](#-regressÃ£o-linear)
2. [ğŸ“Š Naive Bayes](#-naive-bayes)
3. [ğŸŒ³ Ãrvores de DecisÃ£o](#-Ã¡rvores-de-decisÃ£o)
4. [ğŸŒ² Random Forest](#-random-forest)
5. [ğŸ“ KNN - K-Vizinhos Mais PrÃ³ximos](#-knn---k-vizinhos-mais-prÃ³ximos)
6. [ğŸ“¦ KMeans e Clustering](#-kmeans-e-clustering)
7. [ğŸ”— Regras de AssociaÃ§Ã£o (Apriori)](#-regras-de-associaÃ§Ã£o-apriori)

---

## ğŸ“ˆ RegressÃ£o Linear

### IntroduÃ§Ã£o
A regressÃ£o linear Ã© utilizada para modelar a relaÃ§Ã£o entre uma variÃ¡vel dependente contÃ­nua e uma ou mais variÃ¡veis independentes.

### Conceitos:
- **EquaÃ§Ã£o:** \( Y = \beta_0 + \beta_1X + \epsilon \)
- **Objetivo:** Minimizar os erros (diferenÃ§a entre valores previstos e reais) usando o mÃ©todo dos mÃ­nimos quadrados.
- **CondiÃ§Ãµes:** linearidade, normalidade dos resÃ­duos, homocedasticidade, independÃªncia dos erros.

### Labs:
- ğŸ“ `regressao_linear.py`: implementaÃ§Ã£o com `sklearn`
- ğŸ“ `regressao_statsmodels.ipynb`: anÃ¡lise estatÃ­stica completa com `statsmodels`

---

## ğŸ“Š Naive Bayes

### IntroduÃ§Ã£o
Classificador probabilÃ­stico baseado no Teorema de Bayes com suposiÃ§Ã£o de independÃªncia entre os atributos.

### FÃ³rmula:
\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]

### Vantagens:
- RÃ¡pido e eficaz para grandes volumes de dados
- Ideal para problemas de classificaÃ§Ã£o com texto (Spam, Sentimento)

### Labs:
- ğŸ“ `naive_bayes_texto.py`: classificaÃ§Ã£o de texto com `sklearn`
- ğŸ“ `naive_bayes_continuacao.ipynb`: continuaÃ§Ã£o e avaliaÃ§Ã£o de mÃ©tricas

---

## ğŸŒ³ Ãrvores de DecisÃ£o

### IntroduÃ§Ã£o
Modelo baseado em estrutura de Ã¡rvore para decisÃ£o sequencial com base em atributos.

### Conceitos:
- CritÃ©rios de divisÃ£o: **Gini**, **Entropia**
- Cada nÃ³ representa uma pergunta e os ramos, possÃ­veis respostas

### Vantagens:
- FÃ¡cil interpretaÃ§Ã£o
- Pode lidar com variÃ¡veis categÃ³ricas e numÃ©ricas

### Labs:
- ğŸ“ `arvore_decisao.py`: construÃ§Ã£o de Ã¡rvore com `sklearn`
- ğŸ“ `calculo_gini_entropia.ipynb`: (opcional) cÃ¡lculo manual dos critÃ©rios de divisÃ£o

---

## ğŸŒ² Random Forest

### IntroduÃ§Ã£o
Algoritmo de *ensemble learning* baseado na combinaÃ§Ã£o de mÃºltiplas Ã¡rvores de decisÃ£o.

### Conceitos:
- Cada Ã¡rvore Ã© construÃ­da com uma amostra aleatÃ³ria dos dados (bootstrapping)
- A prediÃ§Ã£o final Ã© dada por votaÃ§Ã£o (classificaÃ§Ã£o) ou mÃ©dia (regressÃ£o)

### Vantagens:
- Reduz overfitting
- Alta acurÃ¡cia em muitos problemas reais

### Labs:
- ğŸ“ `random_forest.py`: treino e visualizaÃ§Ã£o da floresta

---

## ğŸ“ KNN - K-Vizinhos Mais PrÃ³ximos

### IntroduÃ§Ã£o
Algoritmo baseado em instÃ¢ncia que classifica um ponto com base na classe majoritÃ¡ria de seus vizinhos mais prÃ³ximos.

### Conceitos:
- DistÃ¢ncia Euclidiana ou outra mÃ©trica
- Valor de K influencia performance (par ou Ã­mpar, pequeno ou grande)

### Vantagens:
- Simples e eficaz
- NÃ£o requer treinamento explÃ­cito

### Labs:
- ğŸ“ `knn.py`: implementaÃ§Ã£o manual e com `sklearn`

---

## ğŸ“¦ KMeans e Clustering

### IntroduÃ§Ã£o
Algoritmo de agrupamento que segmenta os dados em **K clusters** com base na distÃ¢ncia aos centrÃ³ides.

### Conceitos:
- InicializaÃ§Ã£o aleatÃ³ria dos centrÃ³ides
- CritÃ©rio de convergÃªncia: centrÃ³ides estÃ¡veis ou nÃºmero de iteraÃ§Ãµes

### AplicaÃ§Ãµes:
- Agrupamento de clientes, compressÃ£o de imagens, segmentaÃ§Ã£o de mercados

### Labs:
- ğŸ“ `kmeans_aglomerativo_dbscan.py`: comparaÃ§Ã£o entre **KMeans**, **DBSCAN** e **Cluster Aglomerativo**
- ğŸ“ `dendrograma.py`: dendrograma para anÃ¡lise hierÃ¡rquica

---

## ğŸ”— Regras de AssociaÃ§Ã£o (Apriori)

### IntroduÃ§Ã£o
Algoritmo para descoberta de regras de associaÃ§Ã£o em grandes bases de dados transacionais (ex: carrinhos de supermercado).

### Conceitos:
- **Suporte**: frequÃªncia de um item
- **ConfianÃ§a**: probabilidade condicional
- **Lift**: independÃªncia da regra

### Labs:
- ğŸ“ `apriori.py`: regras com `mlxtend`

---

## ğŸ“‚ Requisitos

```bash
pip install numpy pandas matplotlib seaborn scikit-learn statsmodels mlxtend
